---
title: "R_P11_2048046"
author: "R Akhilandeshwari"
date: "4/25/2021"
output:
  word_document: default
  html_document: default
---

<br>

<center>
# SVM 
</center>

<br>

```{r}
setwd("C:/Desktop/R/P11")
getwd()
```

# Loading the packages
```{r}
# set pseudorandom number generator
set.seed(10)

# Attach Packages
library(tidyverse)    # data manipulation and visualization
#install.packages("kernlab")
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
#install.packages("ISLR")
library(ISLR)         # contains example data set "Khan"
library(RColorBrewer) # customized coloring of plots
```

<BR>

# MAXIMAL MARGIN CLASSIFIER
```{r}
# Construct sample data set - completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 3/2
dat <- data.frame(x=x, y=as.factor(y))

# Plot data
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")
```

```{r}
# Fit Support Vector Machine model to data set (e1071)
svmfit <- svm(y~., data = dat, kernel = "linear", scale = FALSE)

# Plot Results
plot(svmfit, dat)
```

```{r}
# fit model and produce plot (kernlab)
kernfit <- ksvm(x, y, type = "C-svc", kernel = 'vanilladot')
plot(kernfit, data = x)
```

<br>

# SUPPORT VECTOR CLASSIFIER
```{r}
# Construct sample data set - not completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
dat <- data.frame(x=x, y=as.factor(y))

# Plot data set
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")
```

```{r}
# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data = dat, kernel = "linear", cost = 10)
# Plot Results
plot(svmfit, dat)
```

```{r}
# Fit Support Vector Machine model to data set
kernfit <- ksvm(x,y, type = "C-svc", kernel = 'vanilladot', C = 100)
# Plot results
plot(kernfit, data = x)
```

```{r}
# find optimal cost of misclassification
tune.out <- tune(svm, y~., data = dat, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
# extract the best model
(bestmod <- tune.out$best.model)
```

```{r}
# Create a table of misclassified observations
ypred <- predict(bestmod, dat)
(misclass <- table(predict = ypred, truth = dat$y))
```

<br>

# SUPPORT VECTOR MACHINES

<BR>

```{r}
# construct larger random data set
x <- matrix(rnorm(200*2), ncol = 2)
x[1:100,] <- x[1:100,] + 2.5
x[101:150,] <- x[101:150,] - 2.5
y <- c(rep(1,150), rep(2,50))
dat <- data.frame(x=x,y=as.factor(y))

# Plot data
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")
```

```{r}
# set pseudorandom number generator
set.seed(123)
# sample training data and fit model
train <- base::sample(200,100, replace = FALSE)
svmfit <- svm(y~., data = dat[train,], kernel = "radial", gamma = 1, cost = 1)
# plot classifier
plot(svmfit, dat)
```

```{r}
# Fit radial-based SVM in kernlab
kernfit <- ksvm(x[train,],y[train], type = "C-svc", kernel = 'rbfdot', C = 1, scaled = c())
# Plot training data
plot(kernfit, data = x[train,])
```

```{r}
# tune model to find optimal cost, gamma values
tune.out <- tune(svm, y~., data = dat[train,], kernel = "radial",
                 ranges = list(cost = c(0.1,1,10,100,1000),
                 gamma = c(0.5,1,2,3,4)))
# show best model
tune.out$best.model
```

```{r}
# validate model performance
(valid <- table(true = dat[-train,"y"], pred = predict(tune.out$best.model,newx = dat[-train,])))
```

<br>

<center><b><u>

# HEART DISEASE CLASSIFICATION USING SVM

</center></b></u>

<br>

## About the data

<br>

*age        ----> Age of patient*
*thalach     ----> maximum heart rate achieved*
*sex	        ----> Sex, 1 for male	*
*exang       ----> exercise induced angina (1 yes)*
*cp	        ----> chest pain*	
*oldpeak     ---->	ST depression induc. ex.*
*trestbps    ---->	resting blood pressure	*
*slope       ---->	slope of peak exercise ST*
*chol        ---->	serum cholesterol	*
*ca          ---->	number of major vessel*
*fbs         ---->	fasting blood sugar larger 120mg/dl (1 true)*
*thal        ---->	no explanation provided, but probably thalassemia (3 normal; 6 fixed defect; 7 reversable defect)*
*restecg     ---->	resting electroc. result (1 anomality)	*
*num         ---->	diagnosis of heart disease (angiographic disease status)*

<br>

```{R}
#Loading the data
heart.data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data",header=FALSE,sep=",",na.strings = '?')

#Renaming the columns of the data
names(heart.data) <- c( "age", "sex", "cp", "trestbps", "chol","fbs", "restecg",
                   "thalach","exang", "oldpeak","slope", "ca", "thal", "num")

#Displaying 1st 3 records of the data
head(heart.data,3)
```

```{r}
dim(heart.data)
```
**There are 14 attributes with 303 records**

<br>

## EDA
```{r}
heart.data$num[heart.data$num > 0] <- 1
barplot(table(heart.data$num),
        main="Fate", col="blue")
```
**The value 0 indicates no heart disease, whereas 1 indicates heart disease**

<br>

```{r}
heart = heart.data #add labels only for plot
heart$num <- as.factor(heart$num)
heart$sex <- as.factor(heart$sex)
levels(heart$num) = c("No disease","Disease")
levels(heart$sex) = c("female","male","")
mosaicplot(heart$sex ~ heart$num,
           main="Fate by Gender", shade=F,color=TRUE,
           xlab="Gender", ylab="Heart disease")
```

<br>

```{r}
boxplot(heart$age ~ heart$num,
        main="Fate by Age",
         ylab="Age",xlab="Heart disease")
```

<br>

```{r}
print(sum(is.na(heart.data)))
heart.data <- na.omit(heart.data)
```
**As there are only 6 null values in the data, I am omitting**

<br>

## Splitting the data
```{r}
library(caret)
library(dplyr)
set.seed(10)
inTrainRows <- createDataPartition(heart.data$num,p=0.7,list=FALSE)
trainData <- heart.data[inTrainRows,]
testData <-  heart.data[-inTrainRows,]
```

<BR>

## BUILDING THE MODEL
```{R}
#Converting the character column to factor
trainData[["num"]] = factor(trainData[["num"]])

#Declaring the control strategy
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

#Building the model
svm_Linear <- train(num ~., data = trainData, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)

#Model Summary
svm_Linear
```
<br>

## Predicting and Validating the model
```{r}
test_pred <- predict(svm_Linear, newdata = testData)
confusionMatrix(table(test_pred, testData$num))
```
<BR>

## Testing our classifier at specific C values

<br><center>

## Tuning the parameter

</center>
```{r}
#Grid search initialisation
grid <- expand.grid(C = c(0,0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2,5))

#Building the model
svm_Linear_Grid <- train(num ~., data = trainData, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneGrid = grid,
tuneLength = 10)

svm_Linear_Grid
```
**The best c value is 0.01**
```{r}
plot(svm_Linear_Grid)
```

<br>

```{r}
test_pred_grid <- predict(svm_Linear_Grid, newdata = testData)
confusionMatrix(table(test_pred_grid, testData$num))
```
**The accuracy for this model is 82%, which is good indicator. Moreover, it is medical data we need to aim at nice sensitivity. In our model the sensitivity is 88% which is good indicator for the model**


